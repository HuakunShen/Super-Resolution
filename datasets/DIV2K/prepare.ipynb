{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbasecondae614ac61c3a74735a88fa4ee5d544264",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preprocess Dataset/Generate Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pathlib\n",
    "import PIL\n",
    "from fastai.vision import *\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIV2K_path = pathlib.Path('.').parent.absolute()"
   ]
  },
  {
   "source": [
    "## Get Image Info"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min dimension of all images=648\n"
     ]
    }
   ],
   "source": [
    "# Training Images\n",
    "train_HR = DIV2K_path/'DIV2K_train_HR'\n",
    "train_image_list = ImageList.from_folder(train_HR)\n",
    "train_image_name_list = [img_path.relative_to(train_HR) for img_path in train_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in train_image_list.items]\n",
    "print(f\"min dimension of all images={torch.min(torch.tensor(shapes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min dimension of all images=816\n"
     ]
    }
   ],
   "source": [
    "# Validation Images\n",
    "valid_HR = DIV2K_path/'DIV2K_valid_HR'\n",
    "valid_image_list = ImageList.from_folder(valid_HR)\n",
    "valid_image_name_list = [img_path.relative_to(valid_HR) for img_path in valid_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in valid_image_list.items]\n",
    "print(f\"min dimension of all images={torch.min(torch.tensor(shapes))}\")"
   ]
  },
  {
   "source": [
    "## Crop HR Images and save"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_res = 600\n",
    "low_res_factor = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HR_crop = DIV2K_path/f'DIV2K_train_HR_crop_{high_res}'\n",
    "train_HR_crop.mkdir(parents=True, exist_ok=True)\n",
    "valid_HR_crop = DIV2K_path/f'DIV2K_valid_HR_crop_{high_res}'\n",
    "valid_HR_crop.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_HR_image_and_save(src_path, dest_path, image_name_list):\n",
    "    for image_name in image_name_list:\n",
    "        src_image_path = src_path/image_name\n",
    "        target_image_path = dest_path/image_name\n",
    "        transformed_img = HR_crop_transforms(PIL.Image.open(src_image_path))\n",
    "        transformed_img.save(target_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HR_Cropper(object):\n",
    "    def __init__(self, src_path, dest_path, high_res):\n",
    "        self.src_path = src_path\n",
    "        self.dest_path = dest_path\n",
    "        self.high_res = high_res\n",
    "\n",
    "    def __call__(self, image_name, i):\n",
    "        src_image_path = self.src_path/image_name\n",
    "        target_image_path = self.dest_path/image_name\n",
    "        src_img = PIL.Image.open(src_image_path)\n",
    "        transformed_img = transforms.Compose([\n",
    "            transforms.RandomCrop(min(src_img.size)),\n",
    "            transforms.RandomCrop(self.high_res, pad_if_needed=True, padding_mode='reflect'),\n",
    "        ])(src_img)\n",
    "        transformed_img.save(target_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {}
    }
   ],
   "source": [
    "# crop and save images\n",
    "parallel(HR_Cropper(valid_HR, valid_HR_crop, high_res), valid_image_name_list)\n",
    "parallel(HR_Cropper(train_HR, train_HR_crop, high_res), train_image_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min dimension of all images=600\n"
     ]
    }
   ],
   "source": [
    "# Cropped Train Images\n",
    "train_HR_crop_image_list = ImageList.from_folder(train_HR_crop)\n",
    "train_HR_crop_image_name_list = [img_path.relative_to(train_HR_crop) for img_path in train_HR_crop_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in train_HR_crop_image_list.items]\n",
    "print(f\"min dimension of all images={torch.min(torch.tensor(shapes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min dimension of all images=600\n"
     ]
    }
   ],
   "source": [
    "# Cropped Train Images\n",
    "valid_HR_crop_image_list = ImageList.from_folder(valid_HR_crop)\n",
    "valid_HR_crop_image_name_list = [img_path.relative_to(valid_HR_crop) for img_path in valid_HR_crop_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in valid_HR_crop_image_list.items]\n",
    "print(f\"min dimension of all images={torch.min(torch.tensor(shapes))}\")"
   ]
  },
  {
   "source": [
    "## Generate Low Resolution Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "low_res_size=150\n"
     ]
    }
   ],
   "source": [
    "low_res_size = high_res // low_res_factor\n",
    "print(f\"low_res_size={low_res_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create destination directory\n",
    "train_LR = DIV2K_path/f'DIV2K_train_LR_{low_res_size}'\n",
    "train_LR.mkdir(parents=True, exist_ok=True)\n",
    "valid_LR = DIV2K_path/f'DIV2K_valid_LR_{low_res_size}'\n",
    "valid_LR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downscaler(object):\n",
    "    def __init__(self, src_path, dest_path, low_res_size):\n",
    "        self.src_path = src_path\n",
    "        self.dest_path = dest_path\n",
    "        self.low_res_size = low_res_size\n",
    "\n",
    "    def __call__(self, image_name, i):\n",
    "        src_image_path = self.src_path/image_name\n",
    "        target_image_path = self.dest_path/image_name\n",
    "        src_img = PIL.Image.open(src_image_path)\n",
    "        downscaled_img = src_img.resize((self.low_res_size, self.low_res_size), resample=PIL.Image.BILINEAR).convert('RGB')\n",
    "        downscaled_img.save(target_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {}
    }
   ],
   "source": [
    "# transform and save images\n",
    "parallel(Downscaler(valid_HR_crop, valid_LR, low_res_size), valid_HR_crop_image_name_list)\n",
    "parallel(Downscaler(train_HR_crop, train_LR, low_res_size), train_HR_crop_image_name_list)"
   ]
  }
 ]
}