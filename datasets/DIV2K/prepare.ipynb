{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbasecondae614ac61c3a74735a88fa4ee5d544264",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preprocess Dataset/Generate Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pathlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import PIL\n",
    "from fastai.vision import *\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIV2K_path = pathlib.Path('.').parent.absolute()\n",
    "train_HR = DIV2K_path/'DIV2K_train_HR'\n",
    "valid_HR = DIV2K_path/'DIV2K_valid_HR'\n",
    "assert DIV2K_path.exists() and train_HR.exists() and valid_HR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "high resolution: 128\nlow resolution: 32\noutput path: /home/hacker/Documents/Super-Resolution/datasets/DIV2K/same_size/128x4\n"
     ]
    }
   ],
   "source": [
    "high_res = 128\n",
    "low_res_factor = 4\n",
    "num_extra = 2\n",
    "same_size = False\n",
    "low_res = high_res // low_res_factor\n",
    "output_path = DIV2K_path/'same_size'/f'{high_res}x{low_res_factor}'\n",
    "print(f'high resolution: {high_res}')\n",
    "print(f'low resolution: {low_res}')\n",
    "print(f'output path: {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and create output_path\n",
    "if output_path.exists():\n",
    "    shutil.rmtree(output_path)\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and create subfolders\n",
    "hr_train = output_path/'hr_train'\n",
    "hr_valid = output_path/'hr_valid'\n",
    "lr_train = output_path/'lr_train'\n",
    "lr_valid = output_path/'lr_valid'\n",
    "for dir_path in [hr_train, hr_valid, lr_train, lr_valid]:\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "source": [
    "## Get Image Info"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min dimension of all training images=648\nmin dimension of all validation images=816\n"
     ]
    }
   ],
   "source": [
    "hr_valid_image_list = ImageList.from_folder(valid_HR)\n",
    "hr_train_image_list = ImageList.from_folder(train_HR)\n",
    "hr_train_image_name_list = [img_path.relative_to(train_HR) for img_path in hr_train_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in hr_train_image_list.items]\n",
    "print(f\"min dimension of all training images={torch.min(torch.tensor(shapes))}\")\n",
    "hr_valid_image_name_list = [img_path.relative_to(valid_HR) for img_path in hr_valid_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in hr_valid_image_list.items]\n",
    "print(f\"min dimension of all validation images={torch.min(torch.tensor(shapes))}\")"
   ]
  },
  {
   "source": [
    "## Image Agumentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_size_center_crop(image, size: int):\n",
    "    return transforms.Compose([\n",
    "        transforms.CenterCrop(min(image.size)),\n",
    "        transforms.Resize(size, interpolation=PIL.Image.BICUBIC)\n",
    "    ])(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAugmentor(object):\n",
    "    def __init__(self, dest_dir, size: int, num_extra:int=2):\n",
    "        assert num_extra <= 5\n",
    "        self.num_extra = num_extra\n",
    "        self.dest_dir = dest_dir\n",
    "        self.size = size\n",
    "        self.base_image, self.image_name, self.filename_no_ext, self.ext = None, None, None, None\n",
    "        self.generated_images, self.generated_image_names = [], []\n",
    "\n",
    "\n",
    "    def generate(self):\n",
    "        random_choices = np.random.choice(a=[1, 2, 3, 4, 5], replace=False, size=self.num_extra)\n",
    "        for choice in random_choices:\n",
    "            if choice == 1:\n",
    "                self.generated_images.append(transforms.functional.hflip(self.base_image))\n",
    "                self.generated_image_names.append(self.filename_no_ext + '-hf' + self.ext)\n",
    "            elif choice == 2:\n",
    "                self.generated_images.append(transforms.functional.vflip(self.base_image))\n",
    "                self.generated_image_names.append(self.filename_no_ext + '-vf' + self.ext)\n",
    "            elif choice == 3:\n",
    "                self.generated_images.append(transforms.functional.rotate(self.base_image, 90))\n",
    "                self.generated_image_names.append(self.filename_no_ext + '-r90' + self.ext)\n",
    "            elif choice == 4:\n",
    "                self.generated_images.append(transforms.functional.rotate(self.base_image, 180))\n",
    "                self.generated_image_names.append(self.filename_no_ext + '-r180' + self.ext)\n",
    "            elif choice == 5:\n",
    "                self.generated_images.append(transforms.functional.rotate(self.base_image, 270))\n",
    "                self.generated_image_names.append(self.filename_no_ext + '-r270' + self.ext)\n",
    "\n",
    "    def save(self):\n",
    "        for i in range(len(self.generated_image_names)):\n",
    "            output_path = self.dest_dir/self.generated_image_names[i]\n",
    "            self.generated_images[i].save(output_path)\n",
    "\n",
    "    def __call__(self, image_path, i):\n",
    "        self.image_name = os.path.basename(image_path)\n",
    "        self.filename_no_ext, self.ext = os.path.splitext(self.image_name)\n",
    "        self.base_image = get_proper_size_center_crop(PIL.Image.open(image_path), self.size)\n",
    "        self.generated_images = [self.base_image]\n",
    "        self.generated_image_names = [self.image_name]\n",
    "        self.generate()\n",
    "        self.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {}
    }
   ],
   "source": [
    "# transform and save images\n",
    "parallel(ImageAugmentor(hr_valid, high_res, num_extra=0), hr_valid_image_list.items)\n",
    "parallel(ImageAugmentor(hr_train, high_res, num_extra=num_extra), hr_train_image_list.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min dimension of all generated HR training images=128\n",
      "min dimension of all generated HR validation images=128\n"
     ]
    }
   ],
   "source": [
    "new_hr_valid_image_list = ImageList.from_folder(hr_valid)\n",
    "new_hr_train_image_list = ImageList.from_folder(hr_train)\n",
    "new_hr_valid_image_names_list = [img_path.relative_to(hr_valid) for img_path in new_hr_valid_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in new_hr_valid_image_list.items]\n",
    "print(f\"min dimension of all generated HR training images={torch.min(torch.tensor(shapes))}\")\n",
    "new_hr_train_image_names_list = [img_path.relative_to(hr_train) for img_path in new_hr_train_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in new_hr_train_image_list.items]\n",
    "print(f\"min dimension of all generated HR validation images={torch.min(torch.tensor(shapes))}\")"
   ]
  },
  {
   "source": [
    "## Generate Low Resolution Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downscaler(object):\n",
    "    def __init__(self, src_path, dest_path, low_res_size, same_size=False):\n",
    "        self.src_path = src_path\n",
    "        self.dest_path = dest_path\n",
    "        self.low_res_size = low_res_size\n",
    "        self.same_size = same_size\n",
    "\n",
    "    def __call__(self, image_name, i):\n",
    "        src_image_path = self.src_path/image_name\n",
    "        target_image_path = self.dest_path/image_name\n",
    "        src_img = PIL.Image.open(src_image_path)\n",
    "        downscaled_img = src_img.resize((self.low_res_size, self.low_res_size), resample=PIL.Image.BICUBIC).convert('RGB')\n",
    "        if self.same_size:\n",
    "            downscaled_img = downscaled_img.resize((src_img.size[0], src_img.size[0]), resample=PIL.Image.BICUBIC).convert('RGB')\n",
    "      \n",
    "        downscaled_img.save(target_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='2400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# transform and save images\n",
    "parallel(Downscaler(hr_valid, lr_valid, low_res, same_size), new_hr_valid_image_names_list)\n",
    "parallel(Downscaler(hr_train, lr_train, low_res, same_size), new_hr_train_image_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}