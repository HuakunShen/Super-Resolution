{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbasecondae614ac61c3a74735a88fa4ee5d544264",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preprocess Dataset/Generate Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pathlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import PIL\n",
    "from fastai.vision import *\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIV2K_path = pathlib.Path('.').parent.absolute()\n",
    "train_HR = DIV2K_path/'DIV2K_train_HR'\n",
    "valid_HR = DIV2K_path/'DIV2K_valid_HR'\n",
    "assert DIV2K_path.exists() and train_HR.exists() and valid_HR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "output path: /home/hacker/Documents/Super-Resolution/datasets/DIV2K/custom/150x4\n"
     ]
    }
   ],
   "source": [
    "high_res = 600\n",
    "low_res_factor = 4\n",
    "low_res = high_res // low_res_factor\n",
    "output_path = DIV2K_path/'custom'/f'{low_res}x4'\n",
    "print(f'output path: {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and create output_path\n",
    "if output_path.exists():\n",
    "    shutil.rmtree(output_path)\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subfolders\n",
    "hr_train = output_path/'hr_train'\n",
    "hr_valid = output_path/'hr_valid'\n",
    "lr_train = output_path/'lr_train'\n",
    "lr_valid = output_path/'lr_valid'\n",
    "for dir_path in [hr_train, hr_valid, lr_train, lr_valid]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "source": [
    "## Get Image Info"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_valid_image_list = ImageList.from_folder(valid_HR)\n",
    "hr_train_image_list = ImageList.from_folder(train_HR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min dimension of all training images=648\nmin dimension of all validation images=816\n"
     ]
    }
   ],
   "source": [
    "hr_train_image_name_list = [img_path.relative_to(train_HR) for img_path in hr_train_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in hr_train_image_list.items]\n",
    "print(f\"min dimension of all training images={torch.min(torch.tensor(shapes))}\")\n",
    "hr_valid_image_name_list = [img_path.relative_to(valid_HR) for img_path in hr_valid_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in hr_valid_image_list.items]\n",
    "print(f\"min dimension of all validation images={torch.min(torch.tensor(shapes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_HR_image_and_save(src_path, dest_path, image_name_list):\n",
    "    for image_name in image_name_list:\n",
    "        src_image_path = src_path/image_name\n",
    "        target_image_path = dest_path/image_name\n",
    "        transformed_img = HR_crop_transforms(PIL.Image.open(src_image_path))\n",
    "        transformed_img.save(target_image_path)"
   ]
  },
  {
   "source": [
    "## Image Agumentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_size_center_crop(image, size: int):\n",
    "    return transforms.Compose([\n",
    "        transforms.CenterCrop(min(image.size)),\n",
    "        transforms.Resize(size, interpolation=PIL.Image.BICUBIC)\n",
    "    ])(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform and save images\n",
    "def test(imagename):\n",
    "    print(imagename)\n",
    "parallel(test, valid_image_name_list)\n",
    "# parallel(ImageAugmentor(train_HR, train_HR_crop, high_res), train_image_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAugmentor(object):\n",
    "    def __init__(self, src_path, dest_dir, size: int, num_extra:int=2):\n",
    "        assert num_extra <= 5\n",
    "        self.num_extra = num_extra\n",
    "        self.src_path = src_path\n",
    "        self.dest_dir = dest_dir\n",
    "        self.size = size\n",
    "        self.image_name = os.path.basename(src_path)\n",
    "        self.filename_no_ext, self.ext = os.path.splitext(self.image_name)\n",
    "        # get base image\n",
    "        img = PIL.Image.open(src_path)\n",
    "        self.base_image = get_proper_size_center_crop(img, size)\n",
    "        self.generated_images = [self.base_image]\n",
    "        self.generated_image_names = [self.image_name]\n",
    "\n",
    "\n",
    "    def generate(self):\n",
    "        random_choices = np.random.choice(a=[1, 2, 3, 4, 5], replace=False, size=self.num_extra)\n",
    "        print(random_choices)\n",
    "        for choice in random_choices:\n",
    "            if choice == 1:\n",
    "                self.generated_images.append(transforms.functional.hflip(self.base_image))                \n",
    "                self.generated_image_names.append(self.filename_no_ext + '-hf' + self.ext)\n",
    "            elif choice == 2:\n",
    "                self.generated_images.append(transforms.functional.vflip(self.base_image))\n",
    "                self.generated_image_names.append(self.filename_no_ext + '-vf' + self.ext)\n",
    "            elif choice == 3:\n",
    "                self.generated_images.append(transforms.functional.rotate(self.base_image, 90))\n",
    "                self.generated_image_names.append(self.filename_no_ext + '-r90' + self.ext)\n",
    "            elif choice == 4:\n",
    "                self.generated_images.append(transforms.functional.rotate(self.base_image, 180))\n",
    "                self.generated_image_names.append(self.filename_no_ext + '-r180' + self.ext)\n",
    "            elif choice == 5:\n",
    "                self.generated_images.append(transforms.functional.rotate(self.base_image, 270))\n",
    "                self.generated_image_names.append(self.filename_no_ext + '-r270' + self.ext)\n",
    "\n",
    "    def save(self):\n",
    "        for i in range(len(self.generated_image_names)):\n",
    "            output_path = self.dest_dir/self.generated_image_names[i]\n",
    "            self.generated_images[i].save(output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAugmentorWrapper(object):\n",
    "    def __init__(self, src_path, dest_path, size, num_extra:int=2):\n",
    "        self.src_path = src_path\n",
    "        self.dest_path = dest_path\n",
    "        self.size = size\n",
    "        self.num_extra = num_extra\n",
    "\n",
    "    def __call__(self, image_name, i):\n",
    "        src_image_path = self.src_path/image_name\n",
    "        target_image_path = self.dest_path/image_name\n",
    "        IA = ImageAugmentor(src_path, dest_dir, size: int, num_extra=self.num_extra)\n",
    "        transformed_img.save(target_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open('/home/hacker/Documents/Super-Resolution/datasets/DIV2K/DIV2K_train_HR/0003.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3 1]\n"
     ]
    }
   ],
   "source": [
    "IA = ImageAugmentor('/home/hacker/Documents/Super-Resolution/datasets/DIV2K/DIV2K_train_HR/0003.png', pathlib.Path('/home/hacker/Documents/Super-Resolution/datasets/DIV2K/custom/150x4/hr_train'), 600)\n",
    "IA.generate()\n",
    "IA.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAug:\n",
    "    def __init__(self, image, size, five_crop=True, n_rand_crop=1, flip=True, rotate=True, angles=[90, 180, 270], hflip=True, vflip=True):\n",
    "        self.size =size\n",
    "        self.angles = angles\n",
    "        self.original_image = image\n",
    "        self.cropped_images = []\n",
    "        self.transformed_images = []\n",
    "        self.five_crop = five_crop\n",
    "        self.n_rand_crop = n_rand_crop\n",
    "        self.hflip = hflip\n",
    "        self.vflip = vflip\n",
    "\n",
    "    def crop(self):\n",
    "        # five crop\n",
    "        if self.five_crop:\n",
    "            self.cropped_images.extend(transforms.Compose([\n",
    "                transforms.FiveCrop(self.size)\n",
    "            ])(self.original_image))\n",
    "        # random crop\n",
    "        for _ in range(n_rand_crop):\n",
    "            self.cropped_images.append(transforms.Compose([\n",
    "                transforms.RandomCrop(min(self.original_image.size)),\n",
    "                transforms.RandomCrop(self.size, pad_if_needed=True, padding_mode='reflect'),\n",
    "            ])(self.original_image))\n",
    "\n",
    "        \n",
    "    def transform(self):\n",
    "        # transform based on existing\n",
    "        for image in self.cropped_images:\n",
    "            # rotate\n",
    "            for angle in self.angles:\n",
    "                self.transformed_images.append(transforms.functional.rotate(image, angle))\n",
    "            # flip\n",
    "            if self.vflip:\n",
    "                self.transformed_images.append(transforms.functional.vflip(image))\n",
    "            if self.hflip:\n",
    "                self.transformed_images.append(transforms.functional.hflip(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HR_Cropper(object):\n",
    "    def __init__(self, src_path, dest_path, high_res):\n",
    "        self.src_path = src_path\n",
    "        self.dest_path = dest_path\n",
    "        self.high_res = high_res\n",
    "\n",
    "    def __call__(self, image_name, i):\n",
    "        src_image_path = self.src_path/image_name\n",
    "        target_image_path = self.dest_path/image_name\n",
    "        src_img = PIL.Image.open(src_image_path)\n",
    "        transformed_img = transforms.Compose([\n",
    "            transforms.RandomCrop(min(src_img.size)),\n",
    "            transforms.RandomCrop(self.high_res, pad_if_needed=True, padding_mode='reflect'),\n",
    "        ])(src_img)\n",
    "        transformed_img.save(target_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# crop and save images\n",
    "parallel(HR_Cropper(valid_HR, valid_HR_crop, high_res), valid_image_name_list)\n",
    "parallel(HR_Cropper(train_HR, train_HR_crop, high_res), train_image_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min dimension of all images=600\n"
     ]
    }
   ],
   "source": [
    "# Cropped Train Images\n",
    "train_HR_crop_image_list = ImageList.from_folder(train_HR_crop)\n",
    "train_HR_crop_image_name_list = [img_path.relative_to(train_HR_crop) for img_path in train_HR_crop_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in train_HR_crop_image_list.items]\n",
    "print(f\"min dimension of all images={torch.min(torch.tensor(shapes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min dimension of all images=600\n"
     ]
    }
   ],
   "source": [
    "# Cropped Train Images\n",
    "valid_HR_crop_image_list = ImageList.from_folder(valid_HR_crop)\n",
    "valid_HR_crop_image_name_list = [img_path.relative_to(valid_HR_crop) for img_path in valid_HR_crop_image_list.items]\n",
    "shapes = [PIL.Image.open(img_path).size for img_path in valid_HR_crop_image_list.items]\n",
    "print(f\"min dimension of all images={torch.min(torch.tensor(shapes))}\")"
   ]
  },
  {
   "source": [
    "## Generate Low Resolution Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "low_res_size=150\n"
     ]
    }
   ],
   "source": [
    "low_res_size = high_res // low_res_factor\n",
    "print(f\"low_res_size={low_res_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create destination directory\n",
    "train_LR = DIV2K_path/'custom'/f'DIV2K_train_LR_{high_res}_{low_res_size}'\n",
    "train_LR.mkdir(parents=True, exist_ok=True)\n",
    "valid_LR = DIV2K_path/'custom'/f'DIV2K_valid_LR_{high_res}_{low_res_size}'\n",
    "valid_LR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downscaler(object):\n",
    "    def __init__(self, src_path, dest_path, low_res_size):\n",
    "        self.src_path = src_path\n",
    "        self.dest_path = dest_path\n",
    "        self.low_res_size = low_res_size\n",
    "\n",
    "    def __call__(self, image_name, i):\n",
    "        src_image_path = self.src_path/image_name\n",
    "        target_image_path = self.dest_path/image_name\n",
    "        src_img = PIL.Image.open(src_image_path)\n",
    "        downscaled_img = src_img.resize((self.low_res_size, self.low_res_size), resample=PIL.Image.BILINEAR).convert('RGB')\n",
    "        downscaled_img.save(target_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {}
    }
   ],
   "source": [
    "# transform and save images\n",
    "parallel(Downscaler(valid_HR_crop, valid_LR, low_res_size), valid_HR_crop_image_name_list)\n",
    "parallel(Downscaler(train_HR_crop, train_LR, low_res_size), train_HR_crop_image_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}